import os
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_openai import OpenAIEmbeddings

# ğŸ”¹ LangChain LLM ì„¤ì •
llm = ChatOpenAI(openai_api_key=os.getenv("OPENAI_API_KEY"), model="gpt-3.5-turbo")

def embed_query(query: str):
    embedder = OpenAIEmbeddings(api_key=os.getenv("OPENAI_API_KEY"))  
    vector = embedder.embed_query(query)
    return [vector]

def generate_reason(query: str, description: str, flower_name: str):
    prompt = PromptTemplate(
        input_variables=["query", "description", "flower"],
        template="""
        ì‚¬ìš©ì ì˜ë„: {query}
        ê½ƒ ì„¤ëª…: {description}
        ì´ ê½ƒì´ '{query}'ì— ì–´ìš¸ë¦¬ëŠ” ì´ìœ ë¥¼ ë‘ ë¬¸ì¥ì´ìƒìœ¼ë¡œ ì„¤ëª…í•´ì¤˜. ê½ƒ ì´ë¦„({flower})ë„ í¬í•¨í•´ì„œ êµ¬ë§¤ìë¥¼ ì¶©ë¶„íˆ ì„¤ë“ í•  ìˆ˜ ìˆë„ë¡ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜.
        """
    )
    chain = LLMChain(llm=llm, prompt=prompt)
    return chain.run({
        "query": query,
        "description": description,
        "flower": flower_name
    }).strip()